{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0ae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies and create environement\n",
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "environment_name = \"CarRacing-v0\"\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple while loop to run the raw random action policy\n",
    "\n",
    "total_episodes = 3\n",
    "for i in range(1, total_episodes+1): #loops through episodes: i is the episode number\n",
    "    obs = env.reset()\n",
    "    over = False\n",
    "    score = 0 \n",
    "    \n",
    "    while over == False:  #loops through timesteps within an episode\n",
    "        env.render()\n",
    "        action = env.action_space.sample() #chooses a random action\n",
    "        obs, reward, over, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{i} Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508292c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() #use this to close the python window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8e4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v0\")\n",
    "env = DummyVecEnv([lambda:env]) #creates a dummy vectorised environment so that original is left unaffected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d30f51",
   "metadata": {},
   "source": [
    "# 1,00,000 Training Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d03eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdirec = os.path.join('Training', 'Logs')\n",
    "model1 = PPO('CnnPolicy', env, verbose=1, tensorboard_log=logdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.learn(100000)#The training details of this model are attached in Repo: i.e. its progress is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb59ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13d9cd06",
   "metadata": {},
   "source": [
    "# 4,30,000 Training Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs and renders a trained model of 4,30,000 steps, zip attached in repo; extract it to same directory as this notebook\n",
    "model2 = PPO.load( 'PPO_430k_Driving_model')\n",
    "evaluate_policy(model2, env, n_eval_episodes=10, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00174c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c3ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RLProj)",
   "language": "python",
   "name": "rlproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
